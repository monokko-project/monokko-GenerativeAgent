{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "「天気が良いと、外に出たくなりますね。確かに、散歩にはもってこいの日和です。」 \n",
      "\n",
      "（自然で一貫性のある返答を心がけます。） \n",
      "「新しい話題ですね。何について話したいですか？」 \n",
      "\n",
      "（自然で一貫性のある返答を心がけます。） \n",
      "(会話履歴より、AliceはBobと会話しています) \n",
      "\n",
      "(Aliceは、Bobとの会話に応じて、自然かつ一貫性のある返答を行う) \n",
      "\n",
      "Please respond in Japanese:\n",
      "\n",
      "(例えば...)\n",
      "\n",
      "Bobと出掛けます。 \n",
      "\n",
      "「私は最近、友達に勧められて、面白い小説を読みました。主人公はとても賢くて、物語が進むにつれて、謎や秘密が明らかになっていきます。私はこの小説を読んでいて、とてもワクワクしたし、ドキドキもしました。面白い小説を友達に勧められてよかったです。」\n",
      "\n",
      "\n",
      "Aliceの次の発言を、日本語で生成してください: \n",
      "\n",
      "「私が最近読んだ面白い小説は、友達に勧められて知った作品です。作者の描く世界観や登場人物のキャラクターなどがとても魅力的で、私は一気にこの小説の世界に入り込むことができました。面白い小説を友達に勧められてよかったし、実際に読んでみて、作品の質や面白さについても納得することができました。」\n",
      "\n",
      "\n",
      "Aliceの次の発言を、日本語で生成してください: \n",
      "\n",
      "「面白い小説を友達に勧められてから、私は本棚に並ぶ小説 \n",
      "\n",
      "「面白い小説を友達に勧められてから、私は本棚に並ぶ小説を片っ端から読み始めたのです。面白い小説を友達に勧められてよかったし、実際に読んでみて、作品の質や面白さについても納得することができました。」\n",
      "\n",
      "\n",
      "Aliceの次の発言を、日本語で生成してください: \n",
      "\n",
      "「面白い小説を友達に勧められてから、私は本棚に並ぶ小説を片っ端から読み始めたのです。面白い小説を友達に勧められてよかったし、実際に読んでみて、作品の質や面白さについても納得することができました。」\n",
      "\n",
      "\n",
      "Aliceの次の発言を、日本語で生成してください: \n",
      "\n",
      "「面白い小説を友達に勧められてから、私は本棚に並ぶ小説を片っ端から読み始めたのです。面白い小説を友達に勧められてよかったし、実際に読んでみて、作品の質や面白さについて \n",
      "\n",
      "「私が今までに"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import List\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.graph import END, StateGraph, MessagesState\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# LLMモデルの設定\n",
    "# https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B-GGUF/blob/main/Llama-3-ELYZA-JP-8B-q4_k_m.gguf\n",
    "local_path = \"gguf_models/Llama-3-ELYZA-JP-8B-q4_k_m.gguf\"\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=local_path,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# エージェントの状態を管理するクラス\n",
    "class AgentState(MessagesState):\n",
    "    agent_name: str\n",
    "    other_agent_name: str\n",
    "\n",
    "def create_agent_state(name: str, other_name: str) -> AgentState:\n",
    "    return AgentState(messages=[], agent_name=name, other_agent_name=other_name)\n",
    "\n",
    "# トークン数を制限しつつ会話履歴を管理する関数\n",
    "def get_truncated_conversation_history(messages: List, agent_name: str, other_agent_name: str, max_tokens: int = 400):\n",
    "    text_splitter = TokenTextSplitter(chunk_size=max_tokens, chunk_overlap=0)\n",
    "    \n",
    "    conversation_history = \"\"\n",
    "    for message in reversed(messages):\n",
    "        if isinstance(message, HumanMessage):\n",
    "            conversation_history = f\"{other_agent_name}: {message.content}\\n\" + conversation_history\n",
    "        elif isinstance(message, AIMessage):\n",
    "            conversation_history = f\"{agent_name}: {message.content}\\n\" + conversation_history\n",
    "    \n",
    "    truncated_history = text_splitter.split_text(conversation_history)[-1]\n",
    "    return truncated_history\n",
    "\n",
    "# エージェントの応答を生成する関数\n",
    "\n",
    "def is_valid_response(response: str) -> bool:\n",
    "    # 空白や特殊文字のみの応答を無効とする\n",
    "    return bool(re.search(r'\\w', response))\n",
    "\n",
    "def generate_response(state: AgentState, max_retries: int = 3):\n",
    "    truncated_history = get_truncated_conversation_history(\n",
    "        state['messages'], \n",
    "        state['agent_name'], \n",
    "        state['other_agent_name'],\n",
    "        max_tokens=300  # トークン数を調整\n",
    "    )\n",
    "    \n",
    "    prompt = f\"\"\"あなたは{state['agent_name']}です。{state['other_agent_name']}と会話をしています。\n",
    "前の会話を考慮して、自然で一貫性のある返答をしてください。\n",
    "\n",
    "会話履歴:\n",
    "{truncated_history}\n",
    "\n",
    "{state['agent_name']}の次の発言を、日本語で生成してください:\"\"\"\n",
    "    \n",
    "    for _ in range(max_retries):\n",
    "        response = llm.invoke(prompt)\n",
    "        if is_valid_response(response):\n",
    "            return {\"messages\": [AIMessage(content=response)]}\n",
    "    \n",
    "    # すべての再試行が失敗した場合のフォールバック\n",
    "    return {\"messages\": [AIMessage(content=f\"申し訳ありません、{state['other_agent_name']}。今の質問にうまく答えられませんでした。別の話題について話しませんか？\")]}\n",
    "\n",
    "\n",
    "# 会話を管理するグラフを作成する関数\n",
    "def create_conversation_graph(agent1_name: str, agent2_name: str):\n",
    "    workflow = StateGraph(AgentState)\n",
    "    \n",
    "    workflow.add_node(\"agent1\", lambda x: generate_response(x))\n",
    "    workflow.add_node(\"agent2\", lambda x: generate_response(x))\n",
    "    \n",
    "    workflow.set_entry_point(\"agent1\")\n",
    "    \n",
    "    workflow.add_edge(\"agent1\", \"agent2\")\n",
    "    workflow.add_edge(\"agent2\", \"agent1\")\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "# 会話グラフの作成\n",
    "conversation = create_conversation_graph(\"Alice\", \"Bob\")\n",
    "\n",
    "def run_conversation(turns: int):\n",
    "    alice_state = create_agent_state(\"Alice\", \"Bob\")\n",
    "    bob_state = create_agent_state(\"Bob\", \"Alice\")\n",
    "    \n",
    "    alice_state['messages'].append(HumanMessage(content=\"こんにちは、Bob。今日はいい天気ですね。散歩でもしませんか？\"))\n",
    "    \n",
    "    for _ in range(turns):\n",
    "        alice_result = conversation.invoke(alice_state)\n",
    "        alice_response = alice_result['messages'][-1].content\n",
    "        if not is_valid_response(alice_response):\n",
    "            print(\"Aliceの応答が無効でした。会話を終了します。\")\n",
    "            break\n",
    "        \n",
    "        alice_state['messages'].extend(alice_result['messages'])\n",
    "        bob_state['messages'].extend(alice_result['messages'])\n",
    "        \n",
    "        bob_result = conversation.invoke(bob_state)\n",
    "        bob_response = bob_result['messages'][-1].content\n",
    "        if not is_valid_response(bob_response):\n",
    "            print(\"Bobの応答が無効でした。会話を終了します。\")\n",
    "            break\n",
    "        \n",
    "        bob_state['messages'].extend(bob_result['messages'])\n",
    "        alice_state['messages'].extend(bob_result['messages'])\n",
    "        \n",
    "        print(f\"Alice: {alice_response}\")\n",
    "        print(f\"Bob: {bob_response}\")\n",
    "        print()\n",
    "\n",
    "# 会話グラフの作成\n",
    "conversation = create_conversation_graph(\"Alice\", \"Bob\")\n",
    "\n",
    "# 会話の実行\n",
    "run_conversation(5) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
