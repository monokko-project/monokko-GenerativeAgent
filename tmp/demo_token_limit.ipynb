{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "「それは素敵なアイデアですね！私も散歩が好きなので、是非一緒に行きましょう」 \n",
      "「では、早速準備を整えますね。私はこの近くに住んでいるのですが、Bobはどちらから来ましたか？」 \n",
      "\n",
      "「ああ、近いということは、私が行きたいと思っている散歩コースもご存知ですよね？実は私、この辺りには珍しい、非常に大きな樹齢数百年の古木があることを知っています。でも、残念ながらその場所を明かすことはできません」 \n",
      "\n",
      "「秘密は守りますが、信じることができないなら、教えることができません」 \n",
      "\n",
      "「理解はしていますが、現状では判断する材料が不足しているため、結論を出すことができません」 \n",
      "\n",
      "会話を続けます。\n",
      "「新しい情報や資料が揃えば、改めて議論することができます」\n",
      "Bobは、Aliceの提案に興味を示しています。どうしますか？ \n",
      "\n",
      "「私もその提案に賛同します。」\n",
      "→ Bobの反応はどのようなものでしょうか？\n",
      "\n",
      "\n",
      "\n",
      "Bobの反応:\n",
      "\n",
      "「ああ、よかった！Aliceも賛成してくれると、安心できるな。」\n",
      "→ 会話が進展し、次のステップを考える必要があります。どうしますか？ \n",
      "\n",
      "\n",
      "\n",
      "「では、具体的にどのように実行する計画を立てる必要がありますね。」\n",
      "→ Bobは、会話の流れに応じて、自然な反応を示すことが重要です。Bobの反応はどのようなものでしょうか？ \n",
      "\n",
      "「うーん、それはあまり良くないと思います。」 \n",
      "\n",
      "Bobが作成した曲名です。\n",
      "Alice: \n",
      "\n",
      "Please go ahead. \n",
      "\n",
      "生成された文章:\n",
      "\n",
      "「具体的にはどんな曲なのでしょうか？タイトルやジャンルなど、教えていただけますか？」 \n",
      "\n",
      "(Note: Bobとの会話履歴を考慮して、自然で一貫性のある返答をします。) \n",
      "Alice:\n",
      "\n",
      "Note: Bobとの会話履歴を考慮して、自然で一貫性のある返答をします。 \n",
      "\n",
      "(前提条件)\n",
      "Bobは、Aliceが作成したプログラムを使用しています。\n",
      "AliceとBobは、協力して、ソフトウェアの開発を進めています。\n",
      "\n",
      "会話の流れ:\n",
      "Bobは、Aliceが作成したプログラムを使用し、問題点や改善提案について、Aliceに相談することがあります。\n",
      "\n",
      "\n",
      "Aliceの返答:\n",
      "Aliceは、Bobの相談に対して、具体的かつ有効な改善策を提示します。また、Aliceは、技術的な専門知識を活用して、Bobの問題点や改善提案について、助言や指導を行うことがあります。\n",
      "\n",
      "\n",
      "日本語で生成した返答:\n",
      "Bobさんが作成されたプログラムを使用し、問題点や改善提案について相談されています。\n",
      "\n",
      "この度は、具体的かつ有効な改善策を提示するべく、技術的な専門知識を活用して助言や指導を行う予定です。 \n",
      "Bobが作成したプログラムに問題点や改善提案がある場合には、具体的かつ有効な改善策を提示するべく、技術的な専門知識を活用して助言や指導を行う予定です。 \n",
      "\n",
      "(翻訳)\n",
      "Bobは、多くの人々が集まって情報交換や議論をし合えるような環境やコミュニティーを構築することが、非常に重要なことであると考えられます。 \n",
      "\n",
      "(Alice) \n",
      "\"Bob, can you summarize the main points of your previous message?\"  \n",
      "(意訳) \n",
      "「Bob、前のメッセージの主要な点を要約することができますか？」\n",
      "\n",
      "\n",
      "Aliceの次の発言を、日本語で生成してください。\n",
      "\n",
      "\n",
      "(Alice) \n",
      "\"I think Bob's point is that, even though we have different opinions and perspectives, we can still find common ground and work together effectively.\"  \n",
      "(意訳) \n",
      "「私は、Bobの主張は、異なる意見や視点があっても、共通の地盤を探し出し、有効に協力することができるということであると考えられます。」 \n",
      "\n",
      "(前提) \n",
      "Aliceは、Bobとの会話を通じて、異なる意見や視点があっても、共通の地盤を探し出し、有効に協力することができるということであると考えられます。\n",
      "\n",
      "(Aliceの次の発言)\n",
      "\n",
      "「私は、Bobの主張を聞いて、以前は異なる意見や視点に基づく反対や否定が存在したとしても、現在は共通の地盤を探し出し、有効に協力することができるということであると考えられます。」 \n",
      "\n",
      "「先ほどの議論を踏まえると、Bobが提案した方法が最も適切な選択肢のように思われます。ただし、この判断は一面的であり、多角的視点から検討する必要があります。」 \n",
      "\n",
      "「この問題について、更に深く掘り下げて考える必要があると考えられます。複雑な要因が絡み合い、容易に解決できない難題であると認識することが肝心です。」 \n",
      "\n",
      "「その通りですね。私たちは常に学び続ける必要がありますが、それはなぜだと思いますか？」 \n",
      "\n",
      "「それは、環境や技術が急速に変化し続けているからだと思います。私たちが学ぶのは、その変化に対応するために、新しい知識やスキルを身につけるためです。」"
     ]
    },
    {
     "ename": "GraphRecursionError",
     "evalue": "Recursion limit of 25 reachedwithout hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mGraphRecursionError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 124\u001b[0m\n\u001b[1;32m    121\u001b[0m conversation \u001b[38;5;241m=\u001b[39m create_conversation_graph(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlice\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBob\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# 会話の実行\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m \u001b[43mrun_conversation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m \n",
      "Cell \u001b[0;32mIn[1], line 98\u001b[0m, in \u001b[0;36mrun_conversation\u001b[0;34m(turns)\u001b[0m\n\u001b[1;32m     95\u001b[0m alice_state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mこんにちは、Bob。今日はいい天気ですね。散歩でもしませんか？\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(turns):\n\u001b[0;32m---> 98\u001b[0m     alice_result \u001b[38;5;241m=\u001b[39m \u001b[43mconversation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43malice_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     alice_response \u001b[38;5;241m=\u001b[39m alice_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_valid_response(alice_response):\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/v2l/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1550\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1549\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1550\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1561\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/v2l/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1133\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m GraphRecursionError(\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecursion limit of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecursion_limit\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m reached\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwithout hitting a stop condition. You can increase the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlimit by setting the `recursion_limit` config key.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1137\u001b[0m     )\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;66;03m# set final channel values as run output\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(read_channels(channels, output_keys))\n",
      "\u001b[0;31mGraphRecursionError\u001b[0m: Recursion limit of 25 reachedwithout hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key."
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import List\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.graph import END, StateGraph, MessagesState\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# LLMモデルの設定\n",
    "# https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B-GGUF/blob/main/Llama-3-ELYZA-JP-8B-q4_k_m.gguf\n",
    "local_path = \"gguf_models/Llama-3-ELYZA-JP-8B-q4_k_m.gguf\"\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=local_path,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# エージェントの状態を管理するクラス\n",
    "class AgentState(MessagesState):\n",
    "    agent_name: str\n",
    "    other_agent_name: str\n",
    "\n",
    "def create_agent_state(name: str, other_name: str) -> AgentState:\n",
    "    return AgentState(messages=[], agent_name=name, other_agent_name=other_name)\n",
    "\n",
    "# トークン数を制限しつつ会話履歴を管理する関数\n",
    "def get_truncated_conversation_history(messages: List, agent_name: str, other_agent_name: str, max_tokens: int = 400):\n",
    "    text_splitter = TokenTextSplitter(chunk_size=max_tokens, chunk_overlap=0)\n",
    "    \n",
    "    conversation_history = \"\"\n",
    "    for message in reversed(messages):\n",
    "        if isinstance(message, HumanMessage):\n",
    "            conversation_history = f\"{other_agent_name}: {message.content}\\n\" + conversation_history\n",
    "        elif isinstance(message, AIMessage):\n",
    "            conversation_history = f\"{agent_name}: {message.content}\\n\" + conversation_history\n",
    "    \n",
    "    truncated_history = text_splitter.split_text(conversation_history)[-1]\n",
    "    return truncated_history\n",
    "\n",
    "# エージェントの応答を生成する関数\n",
    "\n",
    "def is_valid_response(response: str) -> bool:\n",
    "    # 空白や特殊文字のみの応答を無効とする\n",
    "    return bool(re.search(r'\\w', response))\n",
    "\n",
    "def generate_response(state: AgentState, max_retries: int = 3):\n",
    "    truncated_history = get_truncated_conversation_history(\n",
    "        state['messages'], \n",
    "        state['agent_name'], \n",
    "        state['other_agent_name'],\n",
    "        max_tokens=300  # トークン数を調整\n",
    "    )\n",
    "    \n",
    "    prompt = f\"\"\"あなたは{state['agent_name']}です。{state['other_agent_name']}と会話をしています。\n",
    "前の会話を考慮して、自然で一貫性のある返答をしてください。\n",
    "\n",
    "会話履歴:\n",
    "{truncated_history}\n",
    "\n",
    "{state['agent_name']}の次の発言を、日本語で生成してください:\"\"\"\n",
    "    \n",
    "    for _ in range(max_retries):\n",
    "        response = llm.invoke(prompt)\n",
    "        if is_valid_response(response):\n",
    "            return {\"messages\": [AIMessage(content=response)]}\n",
    "    \n",
    "    # すべての再試行が失敗した場合のフォールバック\n",
    "    return {\"messages\": [AIMessage(content=f\"申し訳ありません、{state['other_agent_name']}。今の質問にうまく答えられませんでした。別の話題について話しませんか？\")]}\n",
    "\n",
    "\n",
    "# 会話を管理するグラフを作成する関数\n",
    "def create_conversation_graph(agent1_name: str, agent2_name: str):\n",
    "    workflow = StateGraph(AgentState)\n",
    "    \n",
    "    workflow.add_node(\"agent1\", lambda x: generate_response(x))\n",
    "    workflow.add_node(\"agent2\", lambda x: generate_response(x))\n",
    "    \n",
    "    workflow.set_entry_point(\"agent1\")\n",
    "    \n",
    "    workflow.add_edge(\"agent1\", \"agent2\")\n",
    "    workflow.add_edge(\"agent2\", \"agent1\")\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "# 会話グラフの作成\n",
    "conversation = create_conversation_graph(\"Alice\", \"Bob\")\n",
    "\n",
    "def run_conversation(turns: int):\n",
    "    alice_state = create_agent_state(\"Alice\", \"Bob\")\n",
    "    bob_state = create_agent_state(\"Bob\", \"Alice\")\n",
    "    \n",
    "    alice_state['messages'].append(HumanMessage(content=\"こんにちは、Bob。今日はいい天気ですね。散歩でもしませんか？\"))\n",
    "    \n",
    "    for _ in range(turns):\n",
    "        alice_result = conversation.invoke(alice_state)\n",
    "        alice_response = alice_result['messages'][-1].content\n",
    "        if not is_valid_response(alice_response):\n",
    "            print(\"Aliceの応答が無効でした。会話を終了します。\")\n",
    "            break\n",
    "        \n",
    "        alice_state['messages'].extend(alice_result['messages'])\n",
    "        bob_state['messages'].extend(alice_result['messages'])\n",
    "        \n",
    "        bob_result = conversation.invoke(bob_state)\n",
    "        bob_response = bob_result['messages'][-1].content\n",
    "        if not is_valid_response(bob_response):\n",
    "            print(\"Bobの応答が無効でした。会話を終了します。\")\n",
    "            break\n",
    "        \n",
    "        bob_state['messages'].extend(bob_result['messages'])\n",
    "        alice_state['messages'].extend(bob_result['messages'])\n",
    "        \n",
    "        print(f\"Alice: {alice_response}\")\n",
    "        print(f\"Bob: {bob_response}\")\n",
    "        print()\n",
    "\n",
    "# 会話グラフの作成\n",
    "conversation = create_conversation_graph(\"Alice\", \"Bob\")\n",
    "\n",
    "# 会話の実行\n",
    "run_conversation(5) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
