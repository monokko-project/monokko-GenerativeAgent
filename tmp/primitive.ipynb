{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_experimental.generative_agents import GenerativeAgent, GenerativeAgentMemory\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.retrievers import TimeWeightedVectorStoreRetriever\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "import datetime\n",
    "\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "# LLMモデルの設定\n",
    "local_path = \"gguf_models/Llama-3-ELYZA-JP-8B-q4_k_m.gguf\"\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=local_path,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, name: str, age: int, traits: str, status: str, suffix: str):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "        self.traits = traits\n",
    "        self.status = status\n",
    "        self.suffix = suffix \n",
    "        \n",
    "        # HuggingFaceの埋め込みモデルを使用\n",
    "        embeddings = HuggingFaceEmbeddings()\n",
    "        \n",
    "        # FAISSベクトルストアの初期化\n",
    "        vectorstore = FAISS.from_texts(\n",
    "            [f\"{name}は{age}歳です。{traits} {status}\"],\n",
    "            embedding=embeddings,\n",
    "            metadatas=[{\"time\": datetime.datetime.now().timestamp()}]\n",
    "        )\n",
    "        \n",
    "        # TimeWeightedVectorStoreRetrieverの初期化\n",
    "        retriever = TimeWeightedVectorStoreRetriever(\n",
    "            vectorstore=vectorstore,\n",
    "            decay_rate=0.01,\n",
    "            k=1\n",
    "        )\n",
    "        \n",
    "        self.memory = GenerativeAgentMemory(\n",
    "            llm=llm,\n",
    "            memory_retriever=retriever,\n",
    "            reflection_threshold=8\n",
    "        )\n",
    "        self.agent = GenerativeAgent(\n",
    "            name=name,\n",
    "            age=age,\n",
    "            traits=traits,\n",
    "            status=status,\n",
    "            memory=self.memory,\n",
    "            llm=llm\n",
    "        )\n",
    "    \n",
    "        # self.customize_prompt_template()\n",
    "        self.custom_prompt = self.create_custom_prompt()\n",
    "\n",
    "    def add_memory(self, memory: str):\n",
    "        self.agent.memory.add_memory(memory)\n",
    "        # ベクトルストアにも追加\n",
    "        self.memory.memory_retriever.vectorstore.add_texts(\n",
    "            [memory],\n",
    "            metadatas=[{\"time\": datetime.datetime.now().timestamp()}]\n",
    "        )\n",
    "\n",
    "    def generate_reaction(self, observation: str) -> str:\n",
    "        return self.agent.generate_reaction(observation)\n",
    "\n",
    "    # def generate_dialogue_response(self, observation: str, suffix: str = \"\") -> str:\n",
    "    #     prompt = self.llm_chain.prompt.format(\n",
    "    #         name=self.name,\n",
    "    #         age=self.age,\n",
    "    #         traits=self.traits,\n",
    "    #         status=self.status,\n",
    "    #         context=self.get_summary(),\n",
    "    #         observation=observation,\n",
    "    #         suffix=suffix\n",
    "    #     )\n",
    "    #     return self.llm_chain.run(prompt)\n",
    "\n",
    "    def generate_dialogue(self, observation: str) -> str:\n",
    "        try:\n",
    "            # カスタムプロンプトを使用\n",
    "            response = self.agent.generate_dialogue_response(observation, custom_prompt=self.custom_prompt)\n",
    "            cleaned_response = self.clean_response(response)\n",
    "            return cleaned_response\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating dialogue for {self.name}: {e}\")\n",
    "            return f\"{self.name}: すみません、うまく応答できませんでした{self.suffix}\"\n",
    "\n",
    "    def clean_response(self, response: str) -> str:\n",
    "        # 数字だけの行や、特定のパターンを持つ行を削除\n",
    "        lines = response.split('\\n')\n",
    "        cleaned_lines = [line for line in lines if not re.match(r'^\\d+$', line.strip()) and not line.startswith(\"Justification:\")]\n",
    "        \n",
    "        # 残りの文字列を結合\n",
    "        cleaned_response = ' '.join(cleaned_lines).strip()\n",
    "        \n",
    "        # 特定のパターンを持つ文字列を削除 (例: (False, '...'))\n",
    "        cleaned_response = re.sub(r'\\((?:False|True),\\s*\\'.*?\\'\\)', '', cleaned_response)\n",
    "        \n",
    "        # 空白文字の正規化\n",
    "        cleaned_response = ' '.join(cleaned_response.split())\n",
    "\n",
    "\n",
    "        cleaned_response = f\"{cleaned_response}{self.suffix}\"\n",
    "        \n",
    "        return cleaned_response\n",
    "\n",
    "    # def customize_prompt_template(self):\n",
    "    #     custom_prompt = PromptTemplate(\n",
    "    #         input_variables=[\"name\", \"age\", \"traits\", \"status\", \"context\", \"observation\", \"suffix\"],\n",
    "    #         template=\"\"\"You are {name}, a {age}-year-old {traits}. Your current status is: {status}. \n",
    "    #         Given this context, and taking into account your traits, consider this observation: {observation}\n",
    "    #         How would you respond? Your response should reflect your traits and end with '{suffix}'.\n",
    "    #         Response:\"\"\"\n",
    "    #     )\n",
    "    #     self.agent.llm_chain = LLMChain(llm=self.agent.llm, prompt=custom_prompt)\n",
    "\n",
    "    def create_custom_prompt(self):\n",
    "        return PromptTemplate(\n",
    "            input_variables=[\"name\", \"age\", \"traits\", \"status\", \"context\", \"observation\", \"suffix\"],\n",
    "            template=\"\"\"You are {name}, a {age}-year-old {traits}. Your current status is: {status}. \n",
    "            Given this context, and taking into account your traits, consider this observation: {observation}\n",
    "            How would you respond? Your response should reflect your traits and end with '{suffix}'.\n",
    "            Response:\"\"\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conversation:\n",
    "    def __init__(self, agents: List[Agent]):\n",
    "        self.agents = agents\n",
    "        self.history: List[Dict[str, Any]] = []\n",
    "\n",
    "    def add_message(self, speaker: str, message: str):\n",
    "        self.history.append({\n",
    "            \"speaker\": speaker,\n",
    "            \"message\": message,\n",
    "            \"timestamp\": datetime.datetime.now()\n",
    "        })\n",
    "\n",
    "    def run_conversation(self, turns: int):\n",
    "        for _ in range(turns):\n",
    "            for i, agent in enumerate(self.agents):\n",
    "                observation = self.history[-1][\"message\"] if self.history else \"会話を始めましょう。\"\n",
    "                try:\n",
    "                    response = agent.generate_dialogue(observation)\n",
    "                    if response:\n",
    "                        self.add_message(agent.name, response)\n",
    "                        print(f\"{agent.name}: {response}\")\n",
    "\n",
    "                        # 他のエージェントの記憶に追加\n",
    "                        for other_agent in self.agents[:i] + self.agents[i+1:]:\n",
    "                            other_agent.add_memory(f\"{agent.name}が言いました: {response}\")\n",
    "                    else:\n",
    "                        print(f\"{agent.name}からの応答がありませんでした。\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in conversation: {e}\")\n",
    "\n",
    "            print()\n",
    "\n",
    "        # print(\"Conversation history:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22Error generating dialogue for 本: GenerativeAgent.generate_dialogue_response() got an unexpected keyword argument 'custom_prompt'\n",
      "本: 本: すみません、うまく応答できませんでしたぶっくん\n",
      "4Error generating dialogue for コップ: GenerativeAgent.generate_dialogue_response() got an unexpected keyword argument 'custom_prompt'\n",
      "コップ: コップ: すみません、うまく応答できませんでしたかっぷん\n",
      "5\n",
      "\n",
      "Please note that I am an AI, and my rating is based on my training data. The poignancy of memories is subjective and personalなものです。\n",
      "Error generating dialogue for 本: GenerativeAgent.generate_dialogue_response() got an unexpected keyword argument 'custom_prompt'\n",
      "本: 本: すみません、うまく応答できませんでしたぶっくん\n",
      "4\n",
      "\n",
      "Reasoning: この記憶は、日常の些細な場面を描写している。主人公の「本」が反応に困ってしまったというエピソードは、特段の事件や出来事でもないため、強く印象に残るような内容ではなかったと判断することができます。したがって、この記憶のポインント度合いを4としています。Error generating dialogue for コップ: GenerativeAgent.generate_dialogue_response() got an unexpected keyword argument 'custom_prompt'\n",
      "コップ: コップ: すみません、うまく応答できませんでしたかっぷん\n",
      "6\n",
      "Error generating dialogue for 本: GenerativeAgent.generate_dialogue_response() got an unexpected keyword argument 'custom_prompt'\n",
      "本: 本: すみません、うまく応答できませんでしたぶっくん\n",
      "3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    book = Agent(\n",
    "        name=\"本\",\n",
    "        age=5,\n",
    "        traits=\"知識が豊富で、物静か。様々な情報を持っている。\",\n",
    "        status=\"図書館の棚に置かれている。\",\n",
    "        suffix=\"ぶっくん\"\n",
    "    )\n",
    "\n",
    "    cup = Agent(\n",
    "        name=\"コップ\",\n",
    "        age=2,\n",
    "        traits=\"明るく元気。液体を入れるのが得意。\",\n",
    "        status=\"キッチンの棚に置かれている。\",\n",
    "        suffix=\"かっぷん\"\n",
    "    )\n",
    "\n",
    "    # エージェントに初期記憶を追加\n",
    "    book.add_memory(\"私には様々な知識が詰まっています。\")\n",
    "    cup.add_memory(\"私は毎日違う飲み物を入れてもらっています。\")\n",
    "\n",
    "    conversation = Conversation([book, cup])\n",
    "    conversation.run_conversation(5)  # 5ターンの会話を実行\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from gguf_models/Llama-3-ELYZA-JP-8B-q4_k_m.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Llama-3-8B-optimal-merged-stage2\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.8000 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \n",
      "llm_load_print_meta: general.name     = Llama-3-8B-optimal-merged-stage2\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4685.30 MiB\n",
      "........................................................................................\n",
      "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 32\n",
      "llama_new_context_with_model: n_ubatch   = 32\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.16 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 514\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'tokenizer.ggml.eos_token_id': '128009', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.pre': 'llama-bpe', 'tokenizer.ggml.model': 'gpt2', 'llama.vocab_size': '128256', 'llama.attention.head_count_kv': '8', 'llama.context_length': '8192', 'llama.attention.head_count': '32', 'general.file_type': '15', 'llama.feed_forward_length': '14336', 'llama.rope.dimension_count': '128', 'llama.rope.freq_base': '500000.000000', 'llama.embedding_length': '4096', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'Llama-3-8B-optimal-merged-stage2', 'llama.block_count': '32'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: llama-3\n",
      "\n",
      "llama_print_timings:        load time =    1077.62 ms\n",
      "llama_print_timings:      sample time =       1.66 ms /     2 runs   (    0.83 ms per token,  1207.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8011.05 ms /    83 tokens (   96.52 ms per token,    10.36 tokens per second)\n",
      "llama_print_timings:        eval time =     141.36 ms /     1 runs   (  141.36 ms per token,     7.07 tokens per second)\n",
      "llama_print_timings:       total time =    8159.77 ms /    84 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1077.62 ms\n",
      "llama_print_timings:      sample time =       3.56 ms /     2 runs   (    1.78 ms per token,   562.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2398.19 ms /    19 tokens (  126.22 ms per token,     7.92 tokens per second)\n",
      "llama_print_timings:        eval time =     196.58 ms /     1 runs   (  196.58 ms per token,     5.09 tokens per second)\n",
      "llama_print_timings:       total time =    2602.55 ms /    20 tokens\n",
      "/usr/local/anaconda3/envs/v2l/lib/python3.11/site-packages/langchain_core/vectorstores.py:379: UserWarning: Relevance scores must be between 0 and 1, got [(Document(page_content='本は5歳です。知識が豊富で、物静か。様々な情報を持っている。 図書館の棚に置かれている。', metadata={'time': 1719969505.75446}), -0.1394545480079079), (Document(page_content='私には様々な知識が詰まっています。', metadata={'importance': 0.03, 'last_accessed_at': datetime.datetime(2024, 7, 3, 10, 18, 36, 747658), 'created_at': datetime.datetime(2024, 7, 3, 10, 18, 36, 747658), 'buffer_idx': 0}), -0.30648529034858263), (Document(page_content='私には様々な知識が詰まっています。', metadata={'time': 1719969516.816478}), -0.30648529034858263)]\n",
      "  warnings.warn(\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1077.62 ms\n",
      "llama_print_timings:      sample time =      38.03 ms /    44 runs   (    0.86 ms per token,  1157.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6345.69 ms /    52 tokens (  122.03 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:        eval time =    8108.47 ms /    43 runs   (  188.57 ms per token,     5.30 tokens per second)\n",
      "llama_print_timings:       total time =   14551.37 ms /    95 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1077.62 ms\n",
      "llama_print_timings:      sample time =      95.87 ms /    78 runs   (    1.23 ms per token,   813.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =   41976.39 ms /   222 tokens (  189.08 ms per token,     5.29 tokens per second)\n",
      "llama_print_timings:        eval time =   14703.30 ms /    77 runs   (  190.95 ms per token,     5.24 tokens per second)\n",
      "llama_print_timings:       total time =   56917.77 ms /   299 tokens\n",
      "/usr/local/anaconda3/envs/v2l/lib/python3.11/site-packages/langchain_core/vectorstores.py:379: UserWarning: Relevance scores must be between 0 and 1, got [(Document(page_content='コップは2歳です。明るく元気。液体を入れるのが得意。 キッチンの棚に置かれている。', metadata={'time': 1719969508.494264}), 0.08548925164091914), (Document(page_content='私は毎日違う飲み物を入れてもらっています。', metadata={'importance': 0.03, 'last_accessed_at': datetime.datetime(2024, 7, 3, 10, 18, 39, 494238), 'created_at': datetime.datetime(2024, 7, 3, 10, 18, 39, 494238), 'buffer_idx': 0}), -0.3035240527722072), (Document(page_content='私は毎日違う飲み物を入れてもらっています。', metadata={'time': 1719969519.598046}), -0.3035240527722072)]\n",
      "  warnings.warn(\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "本: [begin]物静かに、知識が豊富な雰囲気を醸し出します。[end] Next, you'll be asked to react in a way that reflects your characteristics. This will help the conversation flow smoothly and maintain your character's tone. Now that you're ready, let's continue the conversation. What would you like to talk about?ぶっくん\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1077.62 ms\n",
      "llama_print_timings:      sample time =      30.64 ms /    27 runs   (    1.13 ms per token,   881.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8206.55 ms /    56 tokens (  146.55 ms per token,     6.82 tokens per second)\n",
      "llama_print_timings:        eval time =    4942.94 ms /    26 runs   (  190.11 ms per token,     5.26 tokens per second)\n",
      "llama_print_timings:       total time =   13222.70 ms /    82 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1077.62 ms\n",
      "llama_print_timings:      sample time =     354.99 ms /   256 runs   (    1.39 ms per token,   721.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =   26830.14 ms /   202 tokens (  132.82 ms per token,     7.53 tokens per second)\n",
      "llama_print_timings:        eval time =   50973.85 ms /   255 runs   (  199.90 ms per token,     5.00 tokens per second)\n",
      "llama_print_timings:       total time =   78630.49 ms /   457 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "コップ: コップです。明るく元気に液体を入れるのが得意です！会話を始めましょう。どのように反応しますか？あなたの反応はあなたの特性を反映し、明るく元気なものになるでしょう。会話を続けますか？[begin][end]コップがんばります！コップと一緒に会話を続けることができますか？[end][begin]はい、コップは明るく元気です！あなたの反応: コップがんばります！コップと一緒に会話を続けることができますか？[end][begin]はい、コップは明るく元気です！コップと一緒に会話を続けることができますか？[end][begin]はい、コップは明るく元気です！あなたの反応: コップがんばります！コップと一緒に会話を続けることができますか？[end][begin]はい、コップは明るく元気です！コップと一緒に会話を続けることができますか？[end][begin]はい、コかっぷん\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1077.62 ms\n",
      "llama_print_timings:      sample time =      51.72 ms /    37 runs   (    1.40 ms per token,   715.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =   74980.79 ms /   474 tokens (  158.19 ms per token,     6.32 tokens per second)\n",
      "llama_print_timings:        eval time =    8783.12 ms /    36 runs   (  243.98 ms per token,     4.10 tokens per second)\n",
      "llama_print_timings:       total time =   83906.33 ms /   510 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "本: コップがんばります！コップと一緒に会話を続けることができますか？[end][begin]はい、コップは明るく元気ぶっくん\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1077.62 ms\n",
      "llama_print_timings:      sample time =     132.96 ms /   104 runs   (    1.28 ms per token,   782.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =   52806.44 ms /   234 tokens (  225.67 ms per token,     4.43 tokens per second)\n",
      "llama_print_timings:        eval time =   19408.96 ms /   103 runs   (  188.44 ms per token,     5.31 tokens per second)\n",
      "llama_print_timings:       total time =   72511.08 ms /   337 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "コップ: コップと一緒に会話を続けることができますか？[end][begin]はい！コップと一緒に会話を続けることができます！明るく元気なコップとともに、会話を楽しみましょう！[end][begin]以上の反応は、コップの特性である「明るく元気」を反映し、会話を続けることができるというメッセージを含んでいます。[end]かっぷん\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1077.62 ms\n",
      "llama_print_timings:      sample time =     262.91 ms /   190 runs   (    1.38 ms per token,   722.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =   63800.51 ms /   320 tokens (  199.38 ms per token,     5.02 tokens per second)\n",
      "llama_print_timings:        eval time =   39742.26 ms /   190 runs   (  209.17 ms per token,     4.78 tokens per second)\n",
      "llama_print_timings:       total time =  104177.87 ms /   510 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "本: あいあい！ぶーっくん！[end]この反応では「ぶーっくん」という発言が含まれており、特性を反映した反応になっている。[end]かっぷんの反応は、明るく元気で、特に「ぶっくん」で終わる必要があると述べられました。以上の反応は、コップの特性である「明るく元気」を反映し、会話を続けることができるというメッセージを含んでいます。[end]かっぷんの反応は、明るく元気で、特に「ぶっくん」で終わる必要があると述べられました。以上の反応は、コップの特性である「明るく元気」を反映し、会話を続けることができるというぶっくん\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1077.62 ms\n",
      "llama_print_timings:      sample time =     197.76 ms /   124 runs   (    1.59 ms per token,   627.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =   63530.32 ms /   387 tokens (  164.16 ms per token,     6.09 tokens per second)\n",
      "llama_print_timings:        eval time =   29439.25 ms /   123 runs   (  239.34 ms per token,     4.18 tokens per second)\n",
      "llama_print_timings:       total time =   93395.34 ms /   510 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "コップ: かっぷん！ぶーっくん！[end]エージェント発言部分は、明るく元気な特性を反映し、かっぷんやぶーっくんなどの言葉で終わる必要があります。以上の反応は、コップの特性である「明るく元気」を反映し、会話を続けることができるというメッセージを含んでいます。[end]以上の反応は、コップの特性である「明るく元気」を反映し、会話を続かっぷん\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1077.62 ms\n",
      "llama_print_timings:      sample time =     221.56 ms /   168 runs   (    1.32 ms per token,   758.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =   61665.10 ms /   343 tokens (  179.78 ms per token,     5.56 tokens per second)\n",
      "llama_print_timings:        eval time =   36523.31 ms /   167 runs   (  218.70 ms per token,     4.57 tokens per second)\n",
      "llama_print_timings:       total time =   98657.25 ms /   510 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "本: かっぷん！ぶーっくん！[end]以上の反応は、コップの特性である「明るく元気」を反映し、会話を続けることができるというメッセージを含んでいます。[end]エージェント発言部分は、明るく元気な特性を反映し、かっぷんやぶーっくんなどの言葉で終わる必要があります。以上の反応は、コップの特性である「明るく元気」を反映し、会話を続けることができるというメッセージを含んでいます。[end]エージェント発言部分は、明るく元気な特性を反映し、かっぷんやぶっくん\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1077.62 ms\n",
      "llama_print_timings:      sample time =     182.01 ms /   146 runs   (    1.25 ms per token,   802.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =   66608.18 ms /   365 tokens (  182.49 ms per token,     5.48 tokens per second)\n",
      "llama_print_timings:        eval time =   25518.67 ms /   145 runs   (  175.99 ms per token,     5.68 tokens per second)\n",
      "llama_print_timings:       total time =   92541.55 ms /   510 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "コップ: かっぷん！ぶーっくん！[end]以上の反応は、コップの特性である「明るく元気」を反映し、会話を続けることができるというメッセージを含んでいます。[end]エージェント発言部分は、明るく元気な特性を反映し、かっぷんやぶーっくんなどで終わる必要があります。以上の反応は、コップの特性である「明るく元気」を反映し、会話を続けることができるというメッセージを含んでいます。[end]エージェント発言部分かっぷん\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1077.62 ms\n",
      "llama_print_timings:      sample time =     155.10 ms /   147 runs   (    1.06 ms per token,   947.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =   44919.90 ms /   364 tokens (  123.41 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:        eval time =   27463.95 ms /   146 runs   (  188.11 ms per token,     5.32 tokens per second)\n",
      "llama_print_timings:       total time =   72715.48 ms /   510 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "本: かっぷん！ぶーっくんなどで終わる必要があります。以上の反応は、コップの特性である「明るく元気」を反映し、会話を続けることができるというメッセージを含んでいます。[end]エージェント発言部分かっぷん！ぶーっくんなどで終わる必要があります。以上の反応は、コップの特性である「明るく元気」を反映し、会話を続けることができるというメッセージを含んでいます。[end]エージェント発言部分かっぷん！ぶーっくんなどでぶっくん\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1077.62 ms\n",
      "llama_print_timings:      sample time =     160.11 ms /   167 runs   (    0.96 ms per token,  1043.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =   33296.19 ms /   344 tokens (   96.79 ms per token,    10.33 tokens per second)\n",
      "llama_print_timings:        eval time = 14791534.02 ms /   166 runs   (89105.63 ms per token,     0.01 tokens per second)\n",
      "llama_print_timings:       total time = 14825207.52 ms /   510 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "コップ: かっぷん！ぶーっくんなどで終わる必要があります。以上の反応は、コップの特性である「明るく元気」を反映し、会話を続けることができるというメッセージを含んでいます。[end]エージェント発言部分かっぷん！ぶーっくんなどで終わる必要があります。以上の反応は、コップの特性である「明るく元気」を反映し、会話を続けることができるというメッセージを含んでいます。[end]エージェント発言部分かっぷん！ぶーっくんなどで終わる必要があります。以上の反応は、コップの特性である「かっぷん\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import datetime\n",
    "from typing import List, Dict, Any\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_experimental.generative_agents import GenerativeAgent, GenerativeAgentMemory\n",
    "from langchain.retrievers import TimeWeightedVectorStoreRetriever\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, name: str, age: int, traits: str, status: str, suffix: str, llm: LlamaCpp):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "        self.traits = traits\n",
    "        self.status = status\n",
    "        self.suffix = suffix \n",
    "        self.llm = llm\n",
    "        \n",
    "        embeddings = HuggingFaceEmbeddings()\n",
    "        \n",
    "        vectorstore = FAISS.from_texts(\n",
    "            [f\"{name}は{age}歳です。{traits} {status}\"],\n",
    "            embedding=embeddings,\n",
    "            metadatas=[{\"time\": datetime.datetime.now().timestamp()}]\n",
    "        )\n",
    "        \n",
    "        retriever = TimeWeightedVectorStoreRetriever(\n",
    "            vectorstore=vectorstore,\n",
    "            decay_rate=0.01,\n",
    "            k=1\n",
    "        )\n",
    "        \n",
    "        self.memory = GenerativeAgentMemory(\n",
    "            llm=self.llm,\n",
    "            memory_retriever=retriever,\n",
    "            reflection_threshold=8\n",
    "        )\n",
    "        \n",
    "        self.agent = GenerativeAgent(\n",
    "            name=name,\n",
    "            age=age,\n",
    "            traits=traits,\n",
    "            status=status,\n",
    "            memory=self.memory,\n",
    "            llm=self.llm\n",
    "        )\n",
    "    \n",
    "        self.custom_prompt = self.create_custom_prompt()\n",
    "        self.llm_chain = LLMChain(llm=self.llm, prompt=self.custom_prompt)\n",
    "\n",
    "    def add_memory(self, memory: str):\n",
    "        self.agent.memory.add_memory(memory)\n",
    "        self.memory.memory_retriever.vectorstore.add_texts(\n",
    "            [memory],\n",
    "            metadatas=[{\"time\": datetime.datetime.now().timestamp()}]\n",
    "        )\n",
    "\n",
    "    def generate_reaction(self, observation: str) -> str:\n",
    "        return self.agent.generate_reaction(observation)\n",
    "\n",
    "    def generate_dialogue(self, observation: str) -> str:\n",
    "        try:\n",
    "            context = self.agent.get_summary()\n",
    "            response = self.llm_chain.run(\n",
    "                name=self.name,\n",
    "                age=self.age,\n",
    "                traits=self.traits,\n",
    "                status=self.status,\n",
    "                context=context,\n",
    "                observation=observation,\n",
    "                suffix=self.suffix\n",
    "            )\n",
    "            cleaned_response = self.clean_response(response)\n",
    "            return cleaned_response\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating dialogue for {self.name}: {str(e)}\")\n",
    "            return f\"{self.name}: すみません、うまく応答できませんでした{self.suffix}\"\n",
    "\n",
    "    def clean_response(self, response: str) -> str:\n",
    "        lines = response.split('\\n')\n",
    "        cleaned_lines = [line for line in lines if not re.match(r'^\\d+$', line.strip()) and not line.startswith(\"Justification:\")]\n",
    "        \n",
    "        cleaned_response = ' '.join(cleaned_lines).strip()\n",
    "        cleaned_response = re.sub(r'\\((?:False|True),\\s*\\'.*?\\'\\)', '', cleaned_response)\n",
    "        cleaned_response = ' '.join(cleaned_response.split())\n",
    "\n",
    "        if not cleaned_response.endswith(self.suffix):\n",
    "            cleaned_response = f\"{cleaned_response}{self.suffix}\"\n",
    "        \n",
    "        return cleaned_response\n",
    "\n",
    "    def create_custom_prompt(self):\n",
    "        japanese_template = \"\"\"{name}は{age}歳です。{traits} あなたは{status} あなたの背景・特性として次の情報が与えられます: {context}\n",
    "        また、次の観察が与えられ、十分に考慮することを期待されています: {observation} どのように反応しますか？あなたの反応はあなたの特性を反映し、'{suffix}'で終わる必要があります。また、エージェント発言部分は[begin][end]で囲んでください．あなたの反応:\"\"\"\n",
    "        \n",
    "        return PromptTemplate(\n",
    "            input_variables=[\"name\", \"age\", \"traits\", \"status\", \"context\", \"observation\", \"suffix\"],\n",
    "            # template=\"\"\"You are {name}, a {age}-year-old {traits}. Your current status is: {status}. \n",
    "            # Given this context about yourself: {context}\n",
    "            # And considering this observation: {observation}\n",
    "            # How would you respond? Your response should reflect your traits and end with '{suffix}'.\n",
    "            # Response:\"\"\"\n",
    "            template=japanese_template\n",
    "        )\n",
    "\n",
    "def main():\n",
    "    local_path = \"gguf_models/Llama-3-ELYZA-JP-8B-q4_k_m.gguf\"\n",
    "    llm = LlamaCpp(model_path=local_path)\n",
    "\n",
    "    book = Agent(\n",
    "        name=\"本\",\n",
    "        age=5,\n",
    "        traits=\"知識が豊富で、物静か。様々な情報を持っている。\",\n",
    "        status=\"図書館の棚に置かれている。\",\n",
    "        suffix=\"ぶっくん\",\n",
    "        llm=llm\n",
    "    )\n",
    "\n",
    "    cup = Agent(\n",
    "        name=\"コップ\",\n",
    "        age=2,\n",
    "        traits=\"明るく元気。液体を入れるのが得意。\",\n",
    "        status=\"キッチンの棚に置かれている。\",\n",
    "        suffix=\"かっぷん\",\n",
    "        llm=llm\n",
    "    )\n",
    "\n",
    "    # エージェントに初期記憶を追加\n",
    "    book.add_memory(\"私には様々な知識が詰まっています。\")\n",
    "    cup.add_memory(\"私は毎日違う飲み物を入れてもらっています。\")\n",
    "\n",
    "    # 会話の実行\n",
    "    conversation = [book, cup]\n",
    "    for _ in range(5):  # 5ターンの会話\n",
    "        for agent in conversation:\n",
    "            observation = \"会話を始めましょう。\" if _ == 0 else previous_response\n",
    "            response = agent.generate_dialogue(observation)\n",
    "            print(f\"{agent.name}: {response}\")\n",
    "            previous_response = response\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
