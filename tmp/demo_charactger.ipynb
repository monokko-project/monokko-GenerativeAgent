{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Note: The conversation is already started, so the response should be a natural continuation of the conversation. \n",
      "「ああ、すみません。実は最近、山登りにはまっていて、その話をしたくて...」 \n",
      "Please go ahead and respond as Alice.\n",
      "\n",
      "Bobと会話をしています。あなたの性格と背景を反映させつつ、自然で一貫性のある返答をしてください。 \n",
      "\n",
      "会話履歴:\n",
      "\n",
      "あなたの性格と背景を反映させつつ、自然で一貫性のある返答をしてください。\n",
      "\n",
      "Bobと会話をしています。\n",
      "\n",
      "会話履歴:\n",
      "\n",
      "\n",
      "\n",
      "「実は最近、山登りにはまっていて、その話をしたくて...」\n",
      "Alice: 申し訳ありません、Bob。今の質問にうまく答えられませんでした。別の話題について話しませんか？\n",
      "Alice: 申し訳ありません、Bob。今の質問にうまく答えられませんでした。別の話題について話しませんか？\n",
      "Alice:  \n",
      "Please go ahead and respond as Alice.\n",
      "\n",
      "Bobと会話をしています。あなたの性格と背景を反映させつつ、自然で一貫性のある返答をしてください。\n",
      "\n",
      "会話履歴:\n",
      "\n",
      "あなたの次の発言を生成してください: \n",
      "\n",
      "会話履歴:\n",
      "\n",
      "\n",
      "\n",
      "「実は最近、山登りにはまっていて、その話をしたくて...」\n",
      "Alice: 申し訳ありません、 \n",
      "\n",
      "\n",
      "\n",
      "「ああ、Bobも外出することが多いんだ。新しいガジェットや道具なんかが好きなんだよ」 \n",
      "\n",
      "会話履歴:\n",
      "\n",
      "\n",
      "\n",
      "「実は最近、山登りにはまっていて、その話をしたくて...」\n",
      "Alice: 申し訳ありません、\n",
      "Alice:  \n",
      "\n",
      "\n",
      "\n",
      "「ああ、Bobも外出することが多いんだ。新しいガジェットや道具なんかが好きなんだよ」\n",
      "\n",
      "\n",
      "Aliceの次の発言を生成してください:"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.graph import END, StateGraph, MessagesState\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "import re\n",
    "\n",
    "# LLMモデルの設定\n",
    "local_path = \"gguf_models/Llama-3-ELYZA-JP-8B-q4_k_m.gguf\"\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=local_path,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# キャラクター設定\n",
    "character_profiles = {\n",
    "    \"Alice\": {\n",
    "        \"personality\": \"明るく社交的で、好奇心旺盛。科学や技術に興味がある。\",\n",
    "        \"background\": \"28歳の女性。IT企業でソフトウェアエンジニアとして働いている。\",\n",
    "        \"interests\": [\"プログラミング\", \"新しいガジェット\", \"SF映画\", \"ハイキング\"]\n",
    "    },\n",
    "    \"Bob\": {\n",
    "        \"personality\": \"落ち着いていて思慮深い。自然や環境問題に関心がある。\",\n",
    "        \"background\": \"32歳の男性。環境NGOで働いており、持続可能な生活に取り組んでいる。\",\n",
    "        \"interests\": [\"環境保護\", \"有機農業\", \"瞑想\", \"クラシック音楽\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "class AgentState(MessagesState):\n",
    "    agent_name: str\n",
    "    other_agent_name: str\n",
    "    character_profile: Dict\n",
    "\n",
    "def create_agent_state(name: str, other_name: str) -> AgentState:\n",
    "    return AgentState(\n",
    "        messages=[],\n",
    "        agent_name=name,\n",
    "        other_agent_name=other_name,\n",
    "        character_profile=character_profiles[name]\n",
    "    )\n",
    "\n",
    "def get_truncated_conversation_history(messages: List, agent_name: str, other_agent_name: str, max_tokens: int = 300):\n",
    "    text_splitter = TokenTextSplitter(chunk_size=max_tokens, chunk_overlap=0)\n",
    "    \n",
    "    conversation_history = \"\"\n",
    "    for message in reversed(messages):\n",
    "        if isinstance(message, HumanMessage):\n",
    "            conversation_history = f\"{other_agent_name}: {message.content}\\n\" + conversation_history\n",
    "        elif isinstance(message, AIMessage):\n",
    "            conversation_history = f\"{agent_name}: {message.content}\\n\" + conversation_history\n",
    "    \n",
    "    truncated_history = text_splitter.split_text(conversation_history)[-1]\n",
    "    return truncated_history\n",
    "\n",
    "def is_valid_response(response: str) -> bool:\n",
    "    return bool(re.search(r'\\w', response))\n",
    "\n",
    "def generate_response(state: AgentState, max_retries: int = 3):\n",
    "    truncated_history = get_truncated_conversation_history(\n",
    "        state['messages'], \n",
    "        state['agent_name'], \n",
    "        state['other_agent_name'],\n",
    "        max_tokens=300\n",
    "    )\n",
    "    \n",
    "    profile = state['character_profile']\n",
    "    prompt = f\"\"\"あなたは{state['agent_name']}として振る舞ってください。以下はあなたの設定です：\n",
    "\n",
    "性格: {profile['personality']}\n",
    "背景: {profile['background']}\n",
    "興味: {', '.join(profile['interests'])}\n",
    "\n",
    "{state['other_agent_name']}と会話をしています。あなたの性格と背景を反映させつつ、自然で一貫性のある返答をしてください。\n",
    "\n",
    "会話履歴:\n",
    "{truncated_history}\n",
    "\n",
    "{state['agent_name']}の次の発言を生成してください:\"\"\"\n",
    "    \n",
    "    for _ in range(max_retries):\n",
    "        response = llm.invoke(prompt)\n",
    "        if is_valid_response(response):\n",
    "            return {\"messages\": [AIMessage(content=response)]}\n",
    "    \n",
    "    return {\"messages\": [AIMessage(content=f\"申し訳ありません、{state['other_agent_name']}。今の質問にうまく答えられませんでした。別の話題について話しませんか？\")]}\n",
    "\n",
    "def create_conversation_graph(agent1_name: str, agent2_name: str):\n",
    "    workflow = StateGraph(AgentState)\n",
    "    \n",
    "    workflow.add_node(\"agent1\", lambda x: generate_response(x))\n",
    "    workflow.add_node(\"agent2\", lambda x: generate_response(x))\n",
    "    \n",
    "    workflow.set_entry_point(\"agent1\")\n",
    "    \n",
    "    workflow.add_edge(\"agent1\", \"agent2\")\n",
    "    workflow.add_edge(\"agent2\", \"agent1\")\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "def run_conversation(turns: int):\n",
    "    alice_state = create_agent_state(\"Alice\", \"Bob\")\n",
    "    bob_state = create_agent_state(\"Bob\", \"Alice\")\n",
    "    \n",
    "    alice_state['messages'].append(HumanMessage(content=\"こんにちは、Bob。最近、何か新しい環境保護の取り組みを始めましたか？\"))\n",
    "    \n",
    "    for _ in range(turns):\n",
    "        alice_result = conversation.invoke(alice_state)\n",
    "        alice_response = alice_result['messages'][-1].content\n",
    "        if not is_valid_response(alice_response):\n",
    "            print(\"Aliceの応答が無効でした。会話を終了します。\")\n",
    "            break\n",
    "        \n",
    "        alice_state['messages'].extend(alice_result['messages'])\n",
    "        bob_state['messages'].extend(alice_result['messages'])\n",
    "        \n",
    "        bob_result = conversation.invoke(bob_state)\n",
    "        bob_response = bob_result['messages'][-1].content\n",
    "        if not is_valid_response(bob_response):\n",
    "            print(\"Bobの応答が無効でした。会話を終了します。\")\n",
    "            break\n",
    "        \n",
    "        bob_state['messages'].extend(bob_result['messages'])\n",
    "        alice_state['messages'].extend(bob_result['messages'])\n",
    "        \n",
    "        print(f\"Alice: {alice_response}\")\n",
    "        print(f\"Bob: {bob_response}\")\n",
    "        print()\n",
    "\n",
    "# 会話グラフの作成\n",
    "conversation = create_conversation_graph(\"Alice\", \"Bob\")\n",
    "\n",
    "# 会話の実行\n",
    "run_conversation(5)  # 5ターンの会話を実行"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
