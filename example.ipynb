{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Literal, TypedDict\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint import MemorySaver\n",
    "from langgraph.graph import END, StateGraph, MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp #https://pypi.org/project/llama-cpp-python/\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# https://huggingface.co/bartowski/Meta-Llama-3-8B-Instruct-GGUF\n",
    "# local_path=\"./Meta-Llama-3-8B-Instruct.Q2_K.gguf\"\n",
    "local_path=\"gguf_models/Llama-3-ELYZA-JP-8B-q4_k_m.gguf\"\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=local_path,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools = [TavilySearchResults(max_results=1)]\n",
    "# tool_node = ToolNode(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function that determines whether to continue or not\n",
    "def should_continue(state: MessagesState) -> Literal[\"tools\", END]:\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # If the LLM makes a tool call, then we route to the \"tools\" node\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    # Otherwise, we stop (reply to the user)\n",
    "    return END\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    messages = state['messages']\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new graph\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "# workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Set the entrypoint as `agent`\n",
    "# This means that this node is the first one called\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# We now add a conditional edge\n",
    "# workflow.add_conditional_edges(\n",
    "#     # First, we define the start node. We use `agent`.\n",
    "#     # This means these are the edges taken after the `agent` node is called.\n",
    "#     \"agent\",\n",
    "#     # Next, we pass in the function that will determine which node is called next.\n",
    "\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "。\n",
      "初めまして、私は作家の「Kaito」と申します。主に短編小説や詩を書いています。私の作品は、日常生活で感じるささいなことから、大きなテーマを探求するまで、幅広いジャンルをカバーしています。\n",
      "\n",
      "以下は、私が書いた短編小説です。\n",
      "\n",
      "タイトル: クマと海辺\n",
      "\n",
      "プロット:\n",
      "\n",
      "クマが海辺に来て、アザラシと友達になります。二人は一緒に浜で遊んだり、潮風に当たったりして過ごします。最終的には、クマは家に帰ることを決め、別れを惜しむアザラシに感謝の言葉を残して、海辺を後にするのでした。\n",
      "\n",
      "私はこの短編小説を通じて、日常生活で感じるささいなことから、大きなテーマを探求するまで、幅広いジャンルをカバーしています。読者は私の作品を通じて、自分自身の内面や外部世界との関係性に気づくことができます。私はそのような気づきや新しい"
     ]
    }
   ],
   "source": [
    "# Initialize memory to persist state between graph runs\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "# Finally, we compile it!\n",
    "# This compiles it into a LangChain Runnable,\n",
    "# meaning you can use it as you would any other runnable.\n",
    "# Note that we're (optionally) passing the memory when compiling the graph\n",
    "app = workflow.compile()\n",
    "\n",
    "# Use the Runnable\n",
    "final_state = app.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"日本語でクマが海辺に行ってアザラシと友達になり、最終的には家に帰るというプロットの短編小説を書いてください。自己紹介をしてください\")]},\n",
    "    config={\"configurable\": {\"thread_id\": 42}}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
